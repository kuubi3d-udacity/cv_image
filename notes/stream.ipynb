{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To generate live captions from a webcam feed using PyTorch, you would need to perform real-time image processing and inference. Here's a basic outline of the steps involved in implementing this functionality:\n",
    "\n",
    "# 1. Import the necessary libraries:\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from model import ImageCaptioningModel  # Your image captioning model\n",
    "\n",
    "\n",
    "# 2. Load the pre-trained image captioning model:\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageCaptioningModel().to(device)\n",
    "model.load_state_dict(torch.load(\"path_to_model_weights\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 3. Set up the webcam capture:\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Use the appropriate index if you have multiple cameras\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "\n",
    "# 4. Define image transformations to preprocess the webcam frames:\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# 5. Create a function to process the webcam frames and generate captions:\n",
    "\n",
    "def generate_caption(frame):\n",
    "    image = transform(frame).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "    caption = \" \".join(outputs)  # Modify based on the output format of your model\n",
    "    return caption\n",
    "\n",
    "\n",
    "# 6. Start the main loop to capture frames, process them, and display the captions:\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Preprocess the frame and generate a caption\n",
    "    caption = generate_caption(frame)\n",
    "    \n",
    "    # Display the caption on the frame\n",
    "    cv2.putText(frame, caption, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    \n",
    "    # Exit the loop on 'q' key press\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#Remember to replace `'path_to_model_weights'` in step 2 with the actual path to your pre-trained model weights. \n",
    "#Also, modify the `generate_caption()` function in step 5 to process the model outputs and create the desired caption format based on the specific implementation of your image captioning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stream the video with captions generated by the previous code to OBS Studio, you can make use of the OBS WebSocket API. Here's an example Python code that establishes a WebSocket connection with OBS Studio and sends the frames with captions for streaming:\n",
    "\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import websocket\n",
    "#from obswebsocket import obsws, events\n",
    "import base64\n",
    "import json\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from model import ImageCaptioningModel  # Your image captioning model\n",
    "\n",
    "# OBS WebSocket configuration\n",
    "OBS_HOST = \"localhost\"\n",
    "OBS_PORT = 4444\n",
    "OBS_PASSWORD = \"your_password\"  # Set your OBS password here\n",
    "\n",
    "# Image captioning model configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_WEIGHTS_PATH = \"path_to_model_weights\"  # Set the path to your model weights\n",
    "FONT_PATH = \"path_to_your_font_file\"  # Set the path to a font file for caption display\n",
    "\n",
    "# Initialize OBS WebSocket connection\n",
    "ws = websocket.WebSocket()\n",
    "ws.connect(f\"ws://{OBS_HOST}:{OBS_PORT}\", password=OBS_PASSWORD)\n",
    "\n",
    "# Load the pre-trained image captioning model\n",
    "model = ImageCaptioningModel().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS_PATH))\n",
    "model.eval()\n",
    "\n",
    "# Set up image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load font for caption display\n",
    "font = ImageFont.truetype(FONT_PATH, size=20)\n",
    "\n",
    "# Define a function to process the webcam frames and generate captions\n",
    "def generate_caption(frame):\n",
    "    image = transform(frame).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "    caption = \" \".join(outputs)  # Modify based on the output format of your model\n",
    "    return caption\n",
    "\n",
    "# Start capturing video from webcam\n",
    "cap = cv2.VideoCapture(0)  # Use the appropriate index if you have multiple cameras\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "'''\n",
    "# 6. Connect to the OBS WebSocket server:\n",
    "\n",
    "ws = obsws(\"localhost\", 4444)  # Replace with your OBS WebSocket server address\n",
    "ws.connect()\n",
    "'''\n",
    "\n",
    "# Start the main loop\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Generate caption for the frame\n",
    "    caption = generate_caption(frame)\n",
    "\n",
    "    # Draw the caption on the frame\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.text((10, 10), caption, fill=(255, 255, 255), font=font)\n",
    "\n",
    "    # Convert the image to base64 for sending via WebSocket\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"JPEG\")\n",
    "    img_data = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Send the frame with caption to OBS Studio for streaming\n",
    "    data = {\n",
    "        \"request-type\": \"SetImageSettings\",\n",
    "        \"source\": \"Webcam\",  # Set the name of your OBS webcam source\n",
    "        \"width\": img.width,\n",
    "        \"height\": img.height,\n",
    "        \"mimeType\": \"image/jpeg\",\n",
    "        \"base64ImageData\": img_data\n",
    "    }\n",
    "    ws.send(json.dumps(data))\n",
    "\n",
    "    # Wait for a short interval\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Release the capture and close the WebSocket connection\n",
    "cap.release()\n",
    "ws.close()\n",
    "\n",
    "'''\n",
    "Before running the code, make sure to replace the following placeholders:\n",
    "\n",
    "- `OBS_HOST`: Set the host where OBS Studio is running (usually \"localhost\" if running on the same machine).\n",
    "- `OBS_PORT`: Set the port on which OBS WebSocket is configured to run (default is 4444).\n",
    "- `OBS_PASSWORD`: Set the password you configured in OBS WebSocket settings.\n",
    "- `MODEL_WEIGHTS_PATH`: Set the path to your pre-trained model weights file.\n",
    "- `FONT_PATH`: Set the path to a font file that will be used for displaying captions on the frames.\n",
    "- `source`: Set the name of your OBS webcam source (the name you gave to the source in OBS Studio).\n",
    "\n",
    "After setting the appropriate values, you can run the script, and it will continuously stream the webcam feed with generated captions to OBS Studio.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To send the output of the previous code to OBS Studio, you can use the OBS WebSockets plugin, which provides a WebSocket server that allows you to control OBS Studio programmatically. Here's an example of how you can modify the previous code to send the generated captions to OBS Studio:\n",
    "\n",
    "# 1. Install the `obs-websocket-py` library:\n",
    "\n",
    "# pip install obs-websocket-py\n",
    "\n",
    "\n",
    "# 2. Import the necessary libraries:\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from obswebsocket import obsws, events\n",
    "from model import ImageCaptioningModel  # Your image captioning model\n",
    "\n",
    "\n",
    "# 3. Load the pre-trained image captioning model (same as before):\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageCaptioningModel().to(device)\n",
    "model.load_state_dict(torch.load(\"path_to_model_weights\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 4. Set up the webcam capture (same as before):\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "\n",
    "# 5. Define image transformations (same as before):\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# 6. Connect to the OBS WebSocket server:\n",
    "\n",
    "ws = obsws(\"localhost\", 4444)  # Replace with your OBS WebSocket server address\n",
    "ws.connect()\n",
    "\n",
    "\n",
    "# 7. Create a function to send the captions to OBS Studio:\n",
    "\n",
    "def send_to_obs(caption):\n",
    "    ws.call(requests.SetTextGDIPlusProperties(\"caption_source_name\", text=caption))\n",
    "\n",
    "# Replace `\"caption_source_name\"` with the actual name of the text source in OBS Studio.\n",
    "\n",
    "# 8. Start the main loop:\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Preprocess the frame and generate a caption\n",
    "    caption = generate_caption(frame)\n",
    "    \n",
    "    # Send the caption to OBS Studio\n",
    "    send_to_obs(caption)\n",
    "    \n",
    "    # Display the caption on the frame (optional)\n",
    "    cv2.putText(frame, caption, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    \n",
    "    # Exit the loop on 'q' key press\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture, close the window, and disconnect from OBS\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "ws.disconnect()\n",
    "\n",
    "\n",
    "# Remember to replace `\"localhost\"` in step 6 with the actual address of your OBS WebSocket server. Also, modify `\"caption_source_name\"` in step 7 with the name of the text source you want to update in OBS Studio."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
